---
title: "Gold Price Forecasting"
author: "Emre Usenmez - github.com/emre-us"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction

This report discusses the various attempts at tackling a challenge of forecasting gold mini futures prices relatively accurately. The gold mini futures prices are measures in Indian Rupee (INR) and the data are obtained from: https://www.kaggle.com/datasets/nisargchodavadiya/daily-gold-price-20152021-time-series?select=Gold+Price.csv which is a time series data frame pre-processed from https://in.investing.com/commodities/gold-mini by Nisarg Chodavadiya.

The Gold Price dataset has 2227 rows and 7 columns. These include Date, Price, Open, High, Low, Volume, and Change. Date starts from 1 January 2014 and ends with 5 August 2022. Price reflects the closing prices for that day, while Open is the opening prices. High and Low gives the highest and lowest prices reached in intra-day trading, and Volume shows the volume of trading for that day. Finally Change shows the percentage change from the previous day's closing price.

In order to determine a successful predictive algorithm, this work first divides the data into training and test set with 80-20% split. It then divides the training data again into training and validation sets with 80-20% split. All the training is done on this second training set (and sometimes smaller subset of this to ensure an average laptop can run some of the algorithms in a reasonable time) and tested on the test set. The most successful algorithm was then chosen to run on the validation set as a final determination of the predictive success. Success was defined as root mean square error (RMSE), which can effectively be interpreted as a typical error in predicting star rating. 

The report is split into 4 areas. Following this Introduction, the Analysis section begins with data exploration, examination, and preparation, and subsequently discusses the definition of sucess for the analysis, and analyses the sucess - or lack thereof - of kNN, user & movie effects, and regularisation approaches. The third section then discusses the modeling results on the validation set and the model performance. Finally the report concludes with a brief summary, its limitations, and suggestions for future work.









# 2. Analysis




## 2a. Data & Packages


Before loading up our data, let us load up packages we will need for this report. These are divided into 3 explanatory categories for convenience: helper packages, packages for plots and graphics, and packages for modelling.

```{r}
#first check if you have the following packages, if not then these will be instaled
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(tibble)) install.packages("tibble")
if(!require(tsibble)) install.packages("tsibble")
if(!require(lubridate)) install.packages("lubridate")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(ggrepel)) install.packages("ggrepel")
if(!require(gridExtra)) install.packages("gridExtra")
if(!require(feasts)) install.packages("feasts")
if(!require(forecast)) install.packages("forecast")
if(!require(reshape2)) install.packages("reshapre2")
if(!require(fable)) install.packages("fable")

#next load up these packages

#helper packages
library(tibble) #to be able to convert data frames to tibble objects
library(tsibble) #tibble objects for time series
library(tidyverse) #for tidy data
library(lubridate) #for dealing with dates
library(reshape2) #for reshaping data to long format-useful in plotting multiple time series in same axis

#packages for plots and graphics
library(ggplot2) #for visualisation
library(ggrepel) #for ensuring labels don't overlap in plots
library(gridExtra) #for plotting graphs in a grid or next to each other
library(feasts) #to be able to use autoplot function with tsibble objects
library(forecast) #to be able to use autoplot function with ts objects

#packages for modelling
library(fable)

```


```{r}
#Load up the data:

gold_df <- read.csv("~/R Projects/Gold-Prices-Forecasting/Gold Price.csv")

```

Let's understand the data:

```{r}
str(gold_df)
```

We can see that it is a 2227 by 7 data frame. It provides the measurements for closing, opening, and highest and lowest prices reached for each day, as well as the volume of trading for that day. Finally it provides the % change from the previous day's closing price. These are set against the daily index starting from 1st of January 2014. We can store this information as a __tsibble__ object for easier analysis.

```{r}
gold_df <- gold_df %>%
  mutate(Date = ymd(Date)) %>% #convert Date column from text to daily object with ymd() or with as_date().
  as_tsibble(index = Date) #convert to tsibble by identifying the index

#check the first few rows of the new tsibble object
head(gold_df)
```

We see that we now have a tsibble object with 7 columns. The [1D] means the index is in 1-day intervals. However, we can see that the 5th of January 2014 has not been included. We can check what gaps we have in the data:

```{r}
#use tsibble package's count_gaps function to see where the gaps are
count_gaps(gold_df)
```

It is apparent that the data is missing the weekends. This would cause problems when we are working with tsibble object, especially when attempting to plot seasonalities or forecasting. So let's fill those missing weekends. For this, we can use the __complete()__ function from __tidyr__ package which is already included in the __tidyverse__ package.

```{r}
gold_df <- gold_df %>% 
  complete(Date = seq.Date(min(Date), max(Date), by="day")) %>% #format is: seq.Date(start date, end date, by = unit of Date)
  fill("Price", "Open", "High", "Low", "Volume", "Chg.") #without fill function, complete() above would only fill the dates but we would get NAs for the rest of the columns corresponding to those weekends. With fill() we copy the same values for each column by the immediately preceding value to the NA.

head(gold_df)
```

Now we can see that those missing dates have been filled. However, top left corner of the output tells us that this is now a tibble object and not a tsibble object. So we need to convert it back to a tsibble object. We can then check if there are any gaps.

```{r}
gold_df <- as_tsibble(gold_df)
count_gaps(gold_df)
```

Finally, lets check the dimensions of the data set that now includes the weekends:

```{r}
dim(gold_df)
```

We can see that now the rows have increased from 2227 to 3139. Naturally, there are still 7 columns.









## 2b. Exploration

The first thing to do in any data analysis task is to plot the data and see if any patterns emerge. We can start by plotting the price behaviour against time to see if there is any seasonality, trend, or cyclic patterns. Seasonality is a pattern in time series that repeats regularly, in a fixed and known period. Trend refers to a pattern of a long-term increase or decrease in the data. Finally, cyclic pattern occurs when the data shows rises and falls, not dissimilar to seasonal in a way but not in a fixed frequency.


```{r}
#we can use autoplot() function which produces an appropriate plot of whatever is passed in the first argument

autoplot(gold_df, Price) + #autoplot recognises gold_df as a time series and produces a time plot
  labs(title = "Gold Mini Futures Daily Closing Prices",
       subtitle = "(INR) From 01.01.14 to 05.08.22")
```

From the price behaviour we can see some strong positive trend. We can also see if percentage change of closing prices from the previous day can yield further information that may not be immediately apparent from the prices plot.

```{r}
autoplot(gold_df, Chg.)
```
There seems to be no trend, seasonality, or cyclic behaviour, nor any strong patterns that can aid in developing a forecasting model. So we will continue with closing prices. From the initial plot we can observe that there does not seem to be a strong seasonality despite having a strong trend. We can further plot the closing prices against "seasons".

```{r}
#gg_season() function from feasts package will help with plotting time series against each season.
gg_season(gold_df, labels = "both") #seasonal periods can be changed by including period="month" command within gg_season function
```

We can see that up to second half of 2019 prices movements were largely flat. From the second half of 2019 we begin to see upward trend. There are strong volatilities in 2020 with a drop in March 2020 and a big jump in August 2020. These movements may have been exacarbated with the uncertainties around Covid policies during this period. We also see a drop until April 2021, though a more gradual than the one in March 2020, before resuming the upward trend. We can also see if there is a trend in volumes traded.

```{r}
gold_df %>% gg_season(Volume)
```

The volumes seem to show strong seasonality but this is likely due to having no trading activity over the weekends. We can see 2020 standing out with large volume movements which seem to coincide with large price movements we observe in the price movements for that year. Let's see if there is any relationship between the two by plotting them against each other for 2020.

```{r}
gold_df %>%
  filter(year(Date) == 2020) %>%
  ggplot(aes(x = Volume, y = Price)) +
  geom_point()

```

There does not seem to be any strong relationship between the volume traded and price level.

Overall, there does not seem to be any clear seasonality trends. Given the data is only from 2014, there may be cyclicality that is stretching further in time that we are not observing.










##2c. Data Preperation

We will begin by splitting our data to training and validation, or holdout, sets. The latter will be held out to test the most successful model. The corresponding training sets consists of gold mini futures closing price observations that occurred prior to those observation(s) in the validation set. We will keep 10% of the most recent data in the validation set.

```{r}
#Split data to training and test sets. We can use filter() or slice() functions for this and I will use both here as a way of illustrating how each can be used. There is also ts_split() function from TSStudio package but I did not see the need of installing that package just for splitting this data. tsibble and fable packages provide all the things we need for this work.

#Test set with slice()
gold_validation_df <- gold_df %>% 
  slice(n() -313:0) #since we have 3139 rows, 10% of data is roughly equivalent to the last 314 rows

#We can see the head of test set
head(gold_validation_df)

#which shows that the test set begins with 26 September 2021

#For training set we can use filter() function to see different syntax
gold_train_df <- gold_df %>%
  filter(Date < "2021-09-26")

#We can check that training set ends on 25th of Sep 2021
tail(gold_train_df)
```

We can also check the dimensions of the training and validation sets:

```{r}
tibble(training_set = dim(gold_train_df),
      test_set = dim(gold_validation_df))
```
and we see that training set has 2825 rows and validation set 314 rows. Both has 7 columns as before.

We will next split the training set further to keep 20% of it as a set we will test the models we train on the remaining 80%. We expect the training data set to have 2260 rows and the test set to have 565 rows.

```{r}
#a simpler approach than using slice() or filter() functions is as follows:
gold_train2 <- gold_train_df[1:2260,]
gold_test <- gold_train_df[-(1:2260),]

#we can check the dimensions of our smaller training set and the test set:
tibble(dim(gold_train2),
       dim(gold_test))
```
We see that we have split the data as we intended to.









## 2d. Definition of Success

In order for us to determine the most successful model, we first need to define what is meant by success. In its simplest form, success for our current purposes mean the smallest forecast error, ie the difference between the forecasted value and the corresponding actual value observed. This can be expressed as:

$$
e_{t+h} = y_{t+h} - \hat{y}_{t+h|T}
$$
where,
$y_t$ is observation in time $t$;
$\hat{y}_t$ is forecast of $y_t$;
$\hat{y}_{t+h|T}$ is forecast of $y_{t+h}$ taking account of training data - ie. an h-step forecast taking into account of all observations up to time T 
training data = ${y_1, ..., y_t}$; and
test data = ${y_{t+1}, y_{t+2},...}$

We can then measure the forecast accuracy by summarising forecast errors. There are, however, different ways of doing this. Broadly, speaking, for point forecasts, the forecast accuracy can be divided into two broad categories - scale-dependent errors whereby the forecast errors are on the same scale as the data, and unit-free non-scale dependent errors.



### 2d.i. Scale-dependent errors
These measurements are useful only if we are not looking to compare between series that involve different units. The two most common ones are mean absolute error (MAE) and root mean squared error (RMSE):

$$
MAE = \sqrt{\frac{1}{N} \sum \left| \hat{y}_{t} - y_{t} \right|^2 }
$$
and

$$
RMSE = \sqrt{\frac{1}{N} \sum \left( \hat{y}_{t} - y_{t} \right)^2 }
$$
where $N$ is the total number of observations.

MAE is particularly popular measurement as it is easier to interpret than RMSE. It also does not emphasise the larger errors as RMSE. Another key difference between the two is that a method that minimises the MAE will lead to forecasts of the __median__, and that minimises RMSE will lead to forecasts of the __mean__. Due to this difference, RMSE is also very commonly adopted.

For non-scale-dependent measures that are a number of approaches including mean absolute percentage error (MAPE), symmetric MAPE (sMAPE), mean absolute scaled error (MASE), and root mean squared scaled error (RMSSE). Since we are not going to use datasets with different units, non-scale-dependent measurments are not needed for this project. For detailed discussions of these see Hyndman and Koehler (2006).










## 2e. Models

We can now begin to build our models using the smaller training set and test their forecasting accuracy against the test set.





### 2e.i. First Model: Establishing the base to compare all models against

Without any prior knowledge, the most basic and simple forecast would to be to predict that all future prices are equal to the average price of the historical data, which is our training data. That is:

$$
\hat{y}_{t+h|T} = \frac{(y_1 +...+ y_t)}{N}
$$

We will use __fable__ package's __model__ function to fit our model to the smaller training data and then forecast it.

```{r}
#use model() function from fable package to fit the model to training data

avg_price_fit <- gold_train2 %>%
  model(Average = MEAN(Price))

```

We will then generate a forecast for the next 565 days that are held out as a test set
```{r}
#Generate forecasts for the next 565 days
avg_fc <- avg_price_fit %>%
  forecast(h = 565)

```

We can then plot our forecast against the actual values:

```{r}
#Plot forecasts against actual values
avg_fc %>%
  autoplot(gold_train2, level = NULL) + #add training set's plot to our forecast. Setting level to NULL means it will not give us the default 95% and 80% intervals.
  autolayer(gold_test, Price, colour = "grey") + #add the layer of actual values from the test set
  labs(title = "Forecast for Gold Mini Closing Price") #add title
```

As can be seen, our forecast of average price is way off the actual values. We can check how badly this model has done in its forecasting accuracy by using the __accuracy()__ function:

```{r}
base_accuracy <- accuracy(avg_fc, gold_test)
base_accuracy

```

From the accuracy calculation we can see that we get some ridiculous RMSE and MAE results for our base model. As we go along RMSEs and MAEs of different methods will be compared, so we can create a table with this result:

```{r}
#as before we can use tibble to create our results table:
fc_accuracy_results <- tibble("Base: Just the Average", base_accuracy[,4:5]) %>%
  set_names(c("Method", "RMSE", "MAE"))
fc_accuracy_results
```












# References
Hyndman, R. J., & Koehler, A. B. (2006). Another look at measures of forecast accuracy. International Journal of Forecasting, 22(4), 679–688
Hyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 13 Sep 2022
