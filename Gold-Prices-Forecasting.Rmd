---
title: "Gold Price Forecasting"
author: "Emre Usenmez - github.com/emre-us"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction

This report discusses the various attempts at tackling a challenge of forecasting gold mini futures prices relatively accurately. The gold mini futures prices are measures in Indian Rupee (INR) and the data are obtained from: https://www.kaggle.com/datasets/nisargchodavadiya/daily-gold-price-20152021-time-series?select=Gold+Price.csv which is a time series data frame pre-processed from https://in.investing.com/commodities/gold-mini by Nisarg Chodavadiya.

The Gold Price dataset has 2227 rows and 7 columns. These include Date, Price, Open, High, Low, Volume, and Change. Date starts from 1 January 2014 and ends with 5 August 2022. Price reflects the closing prices for that day, while Open is the opening prices. High and Low gives the highest and lowest prices reached in intra-day trading, and Volume shows the volume of trading for that day. Finally Change shows the percentage change from the previous day's closing price.

In order to determine a successful predictive algorithm, this work first divides the data into training and test set with 80-20% split. It then divides the training data again into training and validation sets with 80-20% split. All the training is done on this second training set (and sometimes smaller subset of this to ensure an average laptop can run some of the algorithms in a reasonable time) and tested on the test set. The most successful algorithm was then chosen to run on the validation set as a final determination of the predictive success. Success was defined as root mean square error (RMSE), which can effectively be interpreted as a typical error in predicting star rating. 

The report is split into 4 areas. Following this Introduction, the Analysis section begins with data exploration, examination, and preparation, and subsequently discusses the definition of sucess for the analysis, and analyses the sucess - or lack thereof - of kNN, user & movie effects, and regularisation approaches. The third section then discusses the modeling results on the validation set and the model performance. Finally the report concludes with a brief summary, its limitations, and suggestions for future work.









# 2. Analysis




## 2a. Data & Packages


Before loading up our data, let us load up packages we will need for this report. These are divided into 3 explanatory categories for convenience: helper packages, packages for plots and graphics, and packages for modelling.

```{r}
#first check if you have the following packages, if not then these will be instaled
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(tibble)) install.packages("tibble")
if(!require(tsibble)) install.packages("tsibble")
if(!require(lubridate)) install.packages("lubridate")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(ggrepel)) install.packages("ggrepel")
if(!require(gridExtra)) install.packages("gridExtra")
if(!require(feasts)) install.packages("feasts")
if(!require(forecast)) install.packages("forecast")
if(!require(reshape2)) install.packages("reshapre2")
if(!require(fable)) install.packages("fable")

#next load up these packages

#helper packages
library(tibble) #to be able to convert data frames to tibble objects
library(tsibble) #tibble objects for time series
library(tidyverse) #for tidy data
library(lubridate) #for dealing with dates
library(reshape2) #for reshaping data to long format-useful in plotting multiple time series in same axis

#packages for plots and graphics
library(ggplot2) #for visualisation
library(ggrepel) #for ensuring labels don't overlap in plots
library(gridExtra) #for plotting graphs in a grid or next to each other
library(feasts) #to be able to use autoplot function with tsibble objects
library(forecast) #to be able to use autoplot function with ts objects

#packages for modelling
library(fable)

```


```{r}
#Load up the data:

gold_df <- read.csv("~/R Projects/Gold-Prices-Forecasting/Gold Price.csv")

```

Let's understand the data:

```{r}
str(gold_df)
```

We can see that it is a 2227 by 7 data frame. It provides the measurements for closing, opening, and highest and lowest prices reached for each day, as well as the volume of trading for that day. Finally it provides the % change from the previous day's closing price. These are set against the daily index starting from 1st of January 2014. We can store this information as a __tsibble__ object for easier analysis.

```{r}
gold_df <- gold_df %>%
  mutate(Date = ymd(Date)) %>% #convert Date column from text to daily object with ymd() or with as_date().
  as_tsibble(index = Date) #convert to tsibble by identifying the index

#check the first few rows of the new tsibble object
head(gold_df)
```

We see that we now have a tsibble object with 7 columns. The [1D] means the index is in 1-day intervals. However, we can see that the 5th of January 2014 has not been included. We can check what gaps we have in the data:

```{r}
#use tsibble package's count_gaps function to see where the gaps are
count_gaps(gold_df)
```

It is apparent that the data is missing the weekends. This would cause problems when we are working with tsibble object, especially when attempting to plot seasonalities or forecasting. So let's fill those missing weekends. For this, we can use the __complete()__ function from __tidyr__ package which is already included in the __tidyverse__ package.

```{r}
gold_df <- gold_df %>% 
  complete(Date = seq.Date(min(Date), max(Date), by="day")) %>% #format is: seq.Date(start date, end date, by = unit of Date)
  fill("Price", "Open", "High", "Low", "Volume", "Chg.") #without fill function, complete() above would only fill the dates but we would get NAs for the rest of the columns corresponding to those weekends. With fill() we copy the same values for each column by the immediately preceding value to the NA.

head(gold_df)
```

Now we can see that those missing dates have been filled. However, top left corner of the output tells us that this is now a tibble object and not a tsibble object. So we need to convert it back to a tsibble object. We can then check if there are any gaps.

```{r}
gold_df <- as_tsibble(gold_df)
count_gaps(gold_df)
```

Finally, lets check the dimensions of the data set that now includes the weekends:

```{r}
dim(gold_df)
```

We can see that now the rows have increased from 2227 to 3139. Naturally, there are still 7 columns.









## 2b. Exploration

The first thing to do in any data analysis task is to plot the data and see if any patterns emerge. We can start by plotting the price behaviour against time to see if there is any seasonality, trend, or cyclic patterns. Seasonality is a pattern in time series that repeats regularly, in a fixed and known period. Trend refers to a pattern of a long-term increase or decrease in the data. Finally, cyclic pattern occurs when the data shows rises and falls, not dissimilar to seasonal in a way but not in a fixed frequency.


```{r}
#we can use autoplot() function which produces an appropriate plot of whatever is passed in the first argument

autoplot(gold_df, Price) + #autoplot recognises gold_df as a time series and produces a time plot
  labs(title = "Gold Mini Futures Daily Closing Prices",
       subtitle = "(INR) From 01.01.14 to 05.08.22")
```

From the price behaviour we can see some strong positive trend. We can also see if percentage change of closing prices from the previous day can yield further information that may not be immediately apparent from the prices plot.

```{r}
autoplot(gold_df, Chg.)
```
There seems to be no trend, seasonality, or cyclic behaviour, nor any strong patterns that can aid in developing a forecasting model. So we will continue with closing prices. From the initial plot we can observe that there does not seem to be a strong seasonality despite having a strong trend. We can further plot the closing prices against "seasons".

```{r}
#gg_season() function from feasts package will help with plotting time series against each season.
gg_season(gold_df, labels = "both") #seasonal periods can be changed by including period="month" command within gg_season function
```

We can see that up to second half of 2019 prices movements were largely flat. From the second half of 2019 we begin to see upward trend. There are strong volatilities in 2020 with a drop in March 2020 and a big jump in August 2020. These movements may have been exacarbated with the uncertainties around Covid policies during this period. We also see a drop until April 2021, though a more gradual than the one in March 2020, before resuming the upward trend. We can also see if there is a trend in volumes traded.

```{r}
gold_df %>% gg_season(Volume)
```

The volumes seem to show strong seasonality but this is likely due to having no trading activity over the weekends. We can see 2020 standing out with large volume movements which seem to coincide with large price movements we observe in the price movements for that year. Let's see if there is any relationship between the two by plotting them against each other for 2020.

```{r}
gold_df %>%
  filter(year(Date) == 2020) %>%
  ggplot(aes(x = Volume, y = Price)) +
  geom_point()

```

There does not seem to be any strong relationship between the volume traded and price level.

Overall, there does not seem to be any clear seasonality trends. Given the data is only from 2014, there may be cyclicality that is stretching further in time that we are not observing.










##2c. Data Preperation

We will begin by splitting our data to training and validation, or holdout, sets. The latter will be held out to test the most successful model. The corresponding training sets consists of gold mini futures closing price observations that occurred prior to those observation(s) in the validation set. We will keep 10% of the most recent data in the validation set.

```{r}
#Split data to training and test sets. We can use filter() or slice() functions for this and I will use both here as a way of illustrating how each can be used. There is also ts_split() function from TSStudio package but I did not see the need of installing that package just for splitting this data. tsibble and fable packages provide all the things we need for this work.

#Test set with slice()
gold_validation_df <- gold_df %>% 
  slice(n() -313:0) #since we have 3139 rows, 10% of data is roughly equivalent to the last 314 rows

#We can see the head of test set
head(gold_validation_df)

#which shows that the test set begins with 26 September 2021

#For training set we can use filter() function to see different syntax
gold_train_df <- gold_df %>%
  filter(Date < "2021-09-26")

#We can check that training set ends on 25th of Sep 2021
tail(gold_train_df)
```

We can also check the dimensions of the training and validation sets:

```{r}
tibble(training_set = dim(gold_train_df),
      test_set = dim(gold_validation_df))
```
and we see that training set has 2825 rows and validation set 314 rows. Both has 7 columns as before.

We will next split the training set further to keep 20% of it as a set we will test the models we train on the remaining 80%. We expect the training data set to have 2260 rows and the test set to have 565 rows.

```{r}
#a simpler approach than using slice() or filter() functions is as follows:
gold_train2 <- gold_train_df[1:2260,]
gold_test <- gold_train_df[-(1:2260),]

#we can check the dimensions of our smaller training set and the test set:
tibble(dim(gold_train2),
       dim(gold_test))
```
We see that we have split the data as we intended to.









## 2d. Definition of Success

In order for us to determine the most successful model, we first need to define what is meant by success. In its simplest form, success for our current purposes mean the smallest forecast error, ie the difference between the forecasted value and the corresponding actual value observed. This can be expressed as:

$$
e_{t+h} = y_{t+h} - \hat{y}_{t+h|T}
$$
where,
$y_t$ is observation in time $t$;
$\hat{y}_t$ is forecast of $y_t$;
$\hat{y}_{t+h|T}$ is forecast of $y_{t+h}$ taking account of training data - ie. an h-step forecast taking into account of all observations up to time T 
training data = ${y_1, ..., y_t}$; and
test data = ${y_{t+1}, y_{t+2},...}$

We can then measure the forecast accuracy by summarising forecast errors. There are, however, different ways of doing this. Broadly, speaking, for point forecasts, the forecast accuracy can be divided into two broad categories - scale-dependent errors whereby the forecast errors are on the same scale as the data, and unit-free non-scale dependent errors.



### 2d.i. Scale-dependent errors
These measurements are useful only if we are not looking to compare between series that involve different units. The two most common ones are mean absolute error (MAE) and root mean squared error (RMSE):

$$
MAE = \sqrt{\frac{1}{N} \sum \left| \hat{y}_{t} - y_{t} \right|^2 }
$$
and

$$
RMSE = \sqrt{\frac{1}{N} \sum \left( \hat{y}_{t} - y_{t} \right)^2 }
$$
where $N$ is the total number of observations.

MAE is particularly popular measurement as it is easier to interpret than RMSE. It also does not emphasise the larger errors as RMSE. Another key difference between the two is that a method that minimises the MAE will lead to forecasts of the __median__, and that minimises RMSE will lead to forecasts of the __mean__. Due to this difference, RMSE is also very commonly adopted.

For non-scale-dependent measures that are a number of approaches including mean absolute percentage error (MAPE), symmetric MAPE (sMAPE), mean absolute scaled error (MASE), and root mean squared scaled error (RMSSE). Since we are not going to use datasets with different units, non-scale-dependent measurments are not needed for this project. For detailed discussions of these see Hyndman and Koehler (2006).










## 2e. Models

We can now begin to build our models using the smaller training set and test their forecasting accuracy against the test set.





### 2e.i. First Model: Establishing the base to compare all models against

Without any prior knowledge, the most basic and simple forecast would to be to predict that all future prices are equal to the average price of the historical data, which is our training data. That is:

$$
\hat{y}_{t+h|T} = \frac{(y_1 +...+ y_t)}{N}
$$

We will use __fable__ package's __model__ function to fit our model to the smaller training data and then forecast it.

```{r}
#use model() function from fable package to fit the model to training data

avg_price_fit <- gold_train2 %>%
  model(Average = MEAN(Price))

```

We will then generate a forecast for the next 565 days that are held out as a test set
```{r}
#Generate forecasts for the next 565 days
avg_fc <- avg_price_fit %>%
  forecast(h = 565)

```

We can then plot our forecast against the actual values:

```{r}
#Plot forecasts against actual values
avg_fc %>%
  autoplot(gold_train2, level = NULL) + #add training set's plot to our forecast. Setting level to NULL means it will not give us the default 95% and 80% intervals.
  autolayer(gold_test, Price, colour = "grey") + #add the layer of actual values from the test set
  labs(title = "Base Line Forecast for Gold Mini Closing Price") #add title
```

As can be seen, our forecast of average price is way off the actual values. We can check how badly this model has done in its forecasting accuracy by using the __accuracy()__ function:

```{r}
base_accuracy <- accuracy(avg_fc, gold_test)
base_accuracy
```

From the accuracy calculation we can see that we get some ridiculous RMSE and MAE results for our base model. This means our typical error, in RMSE terms, is more than 18,000 INR when predicting gold mini closing price. Note that, due to Jensen's inequality MAE will always be smaller than RMSE. As we go along RMSEs and MAEs of different methods will be compared, so we can create a table with this result:

```{r}
#as before we can use tibble to create our results table:
fc_accuracy_results <- tibble("Base: Just the Average", base_accuracy[,4:5]) %>%
  set_names(c("Method", "RMSE", "MAE"))
fc_accuracy_results
```










### 2e.ii. Second Model: Naive

We can perhaps improve on the previous model if instead of setting our forecast as the average price, we set it as the value of last observation. This is also called naive method and can be expressed as:

$$
\hat{y}_{t+h|T} = y_T
$$
Because a naive forecast is optimal when data follow a random walk, these are also called random walk forecasts (Hyndman and Athanasopoulos, 2021).

We can again fit our model to the smaller training set and forecast it. Then we can assess its accuracy by plotting it and calculating the forecasting errors. 

```{r}
#use model() function from fable package to fit the model to training data

naive_fit <- gold_train2 %>%
  model(Average = NAIVE(Price)) #we can either use NAIVE() function or RW() function for random walk

#Generate forecasts for the next 565 days
naive_fc <- naive_fit %>%
  forecast(h = 565)

#Plot forecasts against actual values
naive_fc %>%
  autoplot(gold_train2, level = NULL) + #add training set's plot to our forecast. Setting level to NULL means it will not give us the default 95% and 80% intervals.
  autolayer(gold_test, Price, colour = "grey") + #add the layer of actual values from the test set
  labs(title = "Naive Forecast for Gold Mini Closing Price") #add title
```

As we see from the plot our forecast of last price is way off the actual values but much better than the forecast of average price. We can check how badly this model has done, or how much better it is compared to just the average:

```{r}
naive_accuracy <- accuracy(naive_fc, gold_test)
naive_accuracy

```

We see that MAE and RMSE are still ridiculously high but considerably better than our base model. We managed to reduce our typical error to little less than 4,500 INR, in terms of RMSE, and just about 3,900 INR in terms of MAE. We can add this to our table:

```{r}
fc_accuracy_results <- bind_rows(fc_accuracy_results,
                                 tibble(Method = "Naive",
                                        naive_accuracy[,4:5]))
fc_accuracy_results
```





### 2e.iii: Third Model: Random Walk with Drift

This is a variation of the naive method whereby the forecasts are not stationary but change overtime, referred to as "drift". Drift is effectively the average of the historical data. This is expressed as:

$$
\hat{y}_{t+h|T} = y_T + \frac{h}{T-1}\sum_{t=2}^{T}{(y_t - y_{t-1})}
$$
This is effectively adding "drift" to the naive model which in a way draws a line between the first and last observations and extrapolating into the future. The expression can be rewritten as:

$$
\hat{y}_{t+h|T} = y_T + h\left(\frac{y_t - y_{t-1}}{T-1}\right)
$$

We can model this just like the previous two, this time we run the command RW() for random walk or NAIVE() as before, and add drift as follows:

```{r}
#use model() function from fable package to fit the model to training data
drift_fit <- gold_train2 %>%
  model(Average = NAIVE(Price ~ drift())) #we can either use NAIVE() function or RW() function for random walk. We then drift the closing price while running random walk

#Generate forecasts for the next 565 days
drift_fc <- drift_fit %>%
  forecast(h = 565)

#Plot forecasts against actual values
drift_fc %>%
  autoplot(gold_train2, level = NULL) + #add training set's plot to our forecast. Setting level to NULL means it will not give us the default 95% and 80% intervals.
  autolayer(gold_test, Price, colour = "grey") + #add the layer of actual values from the test set
  labs(title = "Naive with Drift Forecast for Gold Mini Closing Price") #add title
```

We can see on the plot a better performance. Let's calculate our forecasting error:

```{r}
drift_accuracy <- accuracy(drift_fc, gold_test)
drift_accuracy
```
We see that RMSE is now showing that our typical error is just over 3350 INR, and MAE just over 2580 INR. This is a considerable improvement, though we may still do better. Lets add this result to our table:

```{r}
fc_accuracy_results <- bind_rows(fc_accuracy_results,
                                 tibble(Method = "Naive with Drift",
                                        drift_accuracy[,4:5]))
fc_accuracy_results
```





### 2e.iv. Fourth Model: Naive with Drift and with Cross-Validation

So far we used a smaller training set in order to be able to test each training model on the test set. However, instead of dividing the larger training set we created by separating the validation set, we could use cross-validation whereby we use a series of test sets, each consisting of a single observation. The training set that corresponds to that test set includes only the observations before the observation in the test set. This means, no future observations can be used in constructing the forecast. Since small training sets do not yield any reliable training set, the earliest observations are not treated as test sets. This is sometimes called "evaluation on a rolling forecasting origin".

The average over the test sets then yields the forecasting accuracy. To obtain this, we will first create lots of training set with __stretch_tsibble()__ function.


```{r}
#create many training sets for cross-validation:

drift_cv <- gold_train_df %>% #we begin with the larger training set
  stretch_tsibble(.init = 100, .step = 1) %>%  #we use stretch_tsibble() with an initial training set of 100, increasing the size of successive training sets by 1.
  relocate(Date, Price, .id) #stretching introduces a new column ".id" and mutates it as a last column. We reorder it so it becomes the 3rd column after Date and Price

#check the tail of the new tsibble object we created
tail(drift_cv)
```
The new tsibble object we created has an additional third column called ".id" as we intended. This column provides new key that shows us the various training sets. Accordingly, we created 2726 training sets. We can now model our so far most successful model with cross validation and check the accuracy of this model using the __accuracy()__ function as before:

```{r}
#running the following code will take a few minutes. It may be advisable to get some tea during this time
drift_cv_fc_accuracy <- drift_cv %>%
  model(NAIVE(Price ~ drift())) %>%
  forecast(h = 565) %>%
  accuracy(gold_train_df)

drift_cv_fc_accuracy
```

We actually see that our typical error gets worse with the cross validation approach with RMSE increasing to 5169 and MAE to 3614 INR. We will add this to our table as well:

```{r}
fc_accuracy_results <- bind_rows(fc_accuracy_results,
                                 tibble(Method = "Cross_Validated Naive with Drift",
                                        drift_cv_fc_accuracy[,4:5]))
fc_accuracy_results
```





# 2e.v. Fifth Model: Autoregressive (AR)

Up to now we used simple forecasting tools ranging from average to naive to naive with a drift. In this next model we will try a model that aims to describe autocorrelations in the data. Autocorrelation measures linear relationship between lagged values of a time series. If we denote $l$ number of lags and then autocorrelation coefficient can be expressed as:

$$
autocorrelation.coefficient= r_l = \frac{\sum_{t=l+1}^{T}(y_t - \bar{y}) (y_{t-l} - \bar{y})}{\sum_{t=1}^{T}(y_t - \bar{y})^2}
$$

We can calculate the autocorrelation coefficient for the gold mini futures closing prices with various lags with the __ACF()__ function from the __feasts__ package:

```{r}
#We will limit the lags to a maximum of 7 days for illustrative purposes
gold_train_df %>% ACF(Price, lag_max = 7)

```

The values observed in the "acf" column of this tsibble output corresponds to $r_1, ..., r_7$.

In addition to autocorrelation, we also need to understand the concept of stationarity in order for us to be able to run AR model.




#### 2e.v.I. Stationarity

Stationarity refers to those time series whose statistical properties do not depend on the time at which the series is observed. That is, if ${y_t}$ is a stationary time series, then for all s, the distribution of $(y_t, ..., y_{t+s}$ does not depend on t (Hyndman and Athanasopoulos, 2021). This means, those time series that show trends or seasonality are not stationary, though if they are only showing cyclical behaviour they are likely to be stationary since cycles are not of a fixed length so there are no certainties around when peaks and troughs of cycles will be.

For stationary time series, we would expect the acf values to decline to zero quickly, while non-stationary data decreases slowly. We can see from our acf column above that the gold mini futures price is non-stationary since the drop in autocorrelation values are very slow. They are also very large - almost 1 - and positive which is another confirmation for non-stationarity.

This means we would need transformations to turn the price data to stationary. Logarithms, for example, can help stabilise the variance of the time series. Differencing, on the other hand, can help stabilising the mean of a time series. We can compare the behaviour of autocorrelation coefficients between the price pattern itself, and the differences between the closing price of a given day and the day it immediately precedes:

```{r}
gold_train_df %>%
  ACF(Price) %>%
  autoplot() +
  labs(subtitle = "Closing Price Autocorrelation Coefficients")

gold_train_df %>%
  ACF(difference(Price)) %>% #transform time series by differencing
  autoplot() +
  labs(subtitle = "Changes in Closing Price Autocorrelation Coefficients")
```

We can see that the autocorrelation coefficients decline extremely slowly, while these coefficients are close to zero for 1-day differences, with some random variation. This is called white noise, and there are no apparent autocorrelation behaviour. The blue dashed lines in both graphs show the 95% boundary around 0 within which we would expect 95% of the acf to reside. The 95% boundary is calculated as $±\frac{2}{\sqrt{T}}$  where T is the length of time series. 

We can test whether the differenced time series is stationary by using Ljung-Box test. We could also use Box-Pierce but Ljung-Box test tends to be more accurate (Hyndman and Athanasopoulos, 2021). The Ljung-Box argues that a value $Q^*$ has $\chi^2$ distribution with (L-K) degrees of freedom, where L is the maximum lag being considered, and K is the number of parameters in the model. In our methods we considered so far there are no parameters to K = 0. The value $Q^*$ is calculated as:

$$
Q^* = T(T+2) \sum_{l=1}^{L} (T-l)^{-1} r_l^2
$$

Large values of $Q^*$ and low p-value for not-significance indicate white noise. We can therefore check whether the differencing of gold mini futures prices transform the time series to stationary with the Ljung-Box test:

```{r}
gold_train_df %>%
  mutate(diff_price = difference(Price)) %>% #add a new column with the differences to the tsibble object
  features(diff_price, ljung_box, lag = 7) #use features() function from the feast package to apply the ljung-box test to the new column we created with h=7 
```

Ljung-Box $Q^* statistic is 21.6 for h=7 and has a p-value of 0.003. This suggest that the daily change in the closing price is essentially a random amount which is uncorrelated with that of previous days.

We can also carry out unit root test to see if differencing is necessary for establishing stationarity. We will use Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test (Kwiatkowski et al., 1992), whereby the null hypothesis is that the data are stationary. The test can be conducted using __unitroot_kpss()__ function.

```{r}
#check if differencing is necessary for stationarity
gold_train_df %>%
  features(Price, unitroot_kpss) #features and unitroot_kpss functions from feasts package are applied to the Price column of the training set
```
The output does not give the exact p-value but reports it as 0.01 if it is less than 0.01, and as 0.1 if it is greater than 0.1. In this case, the test statistic of 21.73 is bigger than 1% critical value, so the p-value is less than 0.01, and thus we reject the null hypothesis that data is stationary.

Let's do the same for the differenced data:

```{r}
gold_train_df %>%
  mutate(diff_price = difference(Price)) %>% #add a new column with the differences to the tsibble object
  features(diff_price, unitroot_kpss) #features and unitroot_kpss functions from feasts package are applied to the differenced price column we created
```

This time we get a very small test statistic of 0.21 and the p-value is larger than 0.1. We can conclude that we cannot reject the null hypothesis that the data is stationary.

Next we can determine how many first differences are required as in, is it better in terms of stationarity to difference immediately preceding day, or two days prior etc. The fastest way to do this is to use the unitroot_ndiffs() function from the fable package:

```{r}
gold_train_df %>%
  features(Price, unitroot_ndiffs)
```
We see that 1 difference is sufficient to make data stationary. With this understanding of stationarity, we can start building out AR model.




#### 2e.v.II Autoregressive (AR) Models

An autoregressive model attempts to forecast the gold mini futures price using a linear combination of past prices, which is effectively a regression of the variable against itself. If $p$ represents the total number of linear combinations of past prices, $c$ is a constant, and $\varepsilon_t$ is white noise, then the model can be expressed as:

$$
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \varepsilon_t
$$

This autoregressive model of order of p is referred to as AR(p) model. In interpreting AR(1) model, for example we conclude that:

if $\phi_1 = 0$ and $c = 0$ then $y_t$ is white noise;
if $\phi_1 = 1$ and $c = 0$ then $y_t$ is random walk;
if $\phi_1 = 1$ and $c \neq 0$ then $y_t$ is random walk with drift;
if $phi_1 < 0$ then $y_t$ tends to oscillate around mean.

AR(p) models are usually restricted to stationary data, which means we have to impose some constraints on the values of $\phi$ as follows:

For AR(1) model, $\phi_1$ lies between -1 and 1. That is $ -1< \phi_1 <1 $.
For AR(2) model, $\phi_2$ lies between -1 and 1. That is $ -1< \phi_2 <1 $; the sum of $\phi_1$ and $\phi_2$ has to be less than 1 (ie $ \phi_1 + \phi_2 < 1 $); and so does the difference between $\phi_2$ and $\phi_1$ (ie $ \phi_2 - \phi_1 < 1)

For models of higher order, the restrictions are considerably more complicated and we will not considers order of 3 or higher. We also do not need to impose these restrictions manually since __fable__ package does that for us. We will fit both AR(1) and AR(2) models to our smaller training data and then forecast the next 565 days' price behaviour. We will then choose the better fit between the two to compare it against our test set observed values to determine the forecast accuracy as before.

```{r}
#Use ARIMA() argument within model() function within the fable package
AR_fit <- gold_train2 %>%
  model(AR1 = ARIMA(Price ~ pdq(p = 1, d = 1, q= 0)),
        AR2 = ARIMA(Price ~ pdq(p = 2, d = 1, q= 0))) #ARIMA stands for Autoregressive Integrated Moving Average model. p refers to order for AR, and d refers to differencing we need for stationarity. We are looking at AR(2) model so p is either 1 or 2, and stationarity analysis confirmed that we need differencing of 1, so d=1. q relates to moving average, or MA, and since we are only looking at AR model, we keep q as 0. 

#we then ask for the report of this fit:
report(AR_fit)
```

In order to be able to determine the better of the two models, we will choose the lowest AIC, AICc, or BIS values. These are tools used in selecting predictors, and the abbreviations stand for Akaike's Information Criterion (AIC), Corrected Akaike's Information Criterion (AICc), and Schwarz's Bayesian Information Criterion. For details on these see (Hyndman and Athanasopoulos, 2021).

Based on this assessment AR(2) seems to give a better model though the values are very close for either of them. We can fit this model:

```{r}
AR2_fit <- gold_train2 %>%
  model(ARIMA(Price ~ pdq(p = 2, d=1, q=0)))

report(AR2_fit)
```

which gives us an estimate of:

$$
y_t = 6.6292 - 0.0705 y_{t-1} + 0.0541 y_{t-2}
$$

We can plot the residuals to see if they are white noise:

```{r}
#gg_residuals() function will give us the residual plots and ACF plot

AR2_fit %>%
  gg_tsresiduals()
```
We see that all except 2 residual are within the 95% interval around 0. And both count distribution and the residual plot shows that the residuals are likely to be white noise.

Lets check the forecasting accuracy next. we will first forecast to the next 565 days of our AR(2) fitted model and then plot it against our test set to see how well we did:

```{r}
#forecast using the AR(4) fitted model:
AR_fc <- AR2_fit %>%
  forecast(h=565) 

AR_fc %>%
  autoplot(gold_train2, level = NULL) + #add training set's plot to our forecast. Setting level to NULL means it will not give us the default 95% and 80% intervals.
  autolayer(gold_test, Price, colour = "grey") + #add the layer of actual values from the test set
  labs(title = "AR(2) Forecast for Gold Mini Closing Price") #add title
```

The forecast seems to be somewhat accurate. Let's see how we did in terms of RMSE and MAE

```{r}
AR_fc_accuracy <- accuracy(AR_fc, gold_test)
AR_fc_accuracy
```
We see that our typical error is now about 3362 INR in terms of RMSE, and about 2590 INR in terms of MAE which are almost as good as our naive with drift model. We can add this to our results table:

```{r}
fc_accuracy_results <- bind_rows(fc_accuracy_results,
                                 tibble(Method = "AR(2)",
                                        AR_fc_accuracy[,4:5]))
fc_accuracy_results
```

Next, instead of using past values themselves, we can try using past forecast errors in an approach called Moving Average (MA) to see if that improves our forecasting.










## 2e.vi. Sixth Model: Moving Average (MA)

The MA model uses past forecast errors in a regression-like linear model. If $q$ is the order of past forecast errors that are white noise $(\varepsilon_t)$, the moving approach model, referred to as MA(q) model, is expressed as:

$$
y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} +...+ \theta_q \varepsilon_{t-q}
$$

Similar to AR(p) model, MA(q) also has constraints around its $\theta$ as follows:

Absolute value of first theta, which is the coefficient of the white noise error observed from one period earlier's forcast has to be less than 1, that is $ |\theta_1| < 1 $. This is because if it is exactly 1, this means all the error observations have the same weight, or if it is higher than 1, then the distant observations have more influence than the more recent ones which does not really make sense. For $\theta$ in MA(1) and MA(2) models we have similar constraints as we do for $\phi$ in AR(1) and AR(2) models.

As before, we will fit the MA(q) model to our smaller training data using __fable__ package and then forecast the next 565 days' price behaviour. We will then compare it against our test set observed values to determine the forecast accuracy.

```{r}
#Use ARIMA() argument within model() function within the fable package
MA_fit <- gold_train2 %>%
  model(MA1 = ARIMA(Price ~ pdq(p=0, d=1, q=1)), #this time we will leave p as 0 and try q = 1 and q=2, with differencing the price data by 1 day, ie d=1.
        MA2 = ARIMA(Price ~ pdq(p=0, d=1, q=2)))

#we then ask for the report of this fit:
report(MA_fit)
```

The AIC, AICc, and BIC values are almost the same but AICc values are lower for MA(2) model despite BIC being higher. Hyndman and Athanasopoulos (2021) argue that AICc is a better metric to use in determining which model is better, so we will adopt MA(2) in our approach here. We can fit this model as follows:

```{r}
MA2_fit <- gold_train2 %>%
  model(ARIMA(Price ~ pdq(p=0, d=1, q=2)))

report(MA2_fit)
```

The fitted model is therefore:

$$
y_t = 6.3908 + 0.0227 - 0.0653 \varepsilon_{t-1} + 0.0519 \varepsilon_{t-2}
$$

We can plot the residuals to see if they are white noise:

```{r}
#gg_residuals() function will give us the residual plots and ACF plot

MA2_fit %>%
  gg_tsresiduals()
```

We see that all except 2 residual are within the 95% interval around 0. And both count distribution and the residual plot shows that the residuals are likely to be white noise.

Lets check the forecasting accuracy next. we will first forecast to the next 565 days of our MA(2) fitted model and then plot it against our test set to see how well we did:

```{r}
#forecast using the AR(4) fitted model:
MA_fc <- MA2_fit %>%
  forecast(h=565) 

MA_fc %>%
  autoplot(gold_train2, level = NULL) + #add training set's plot to our forecast. Setting level to NULL means it will not give us the default 95% and 80% intervals.
  autolayer(gold_test, Price, colour = "grey") + #add the layer of actual values from the test set
  labs(title = "MA(2) Forecast for Gold Mini Closing Price") #add title
```

The forecast seems to be somewhat accurate. Let's see how we did in terms of RMSE and MAE

```{r}
MA_fc_accuracy <- accuracy(MA_fc, gold_test)
MA_fc_accuracy
```
We see that our typical error has improved to about 3332 INR in terms of RMSE, and to about 2569 INR in terms of MAE which are better than our naive with drift model. We can add this to our results table:

```{r}
fc_accuracy_results <- bind_rows(fc_accuracy_results,
                                 tibble(Method = "MA(2)",
                                        MA_fc_accuracy[,4:5]))
fc_accuracy_results
```

We can next try combining AR and MA methods to see if we have an improvement.









### 2e.vii. Seventh Model: Autoregressive Integrated Moving Average (ARIMA) Model

If we represent the differenced series as $y'_t$ then the combination of AR and MA models can be expressed as:

$$
y'_t = c + \phi_1 y'_{t-1} + ... + \phi_p y'_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} +...+ \theta_q \varepsilon_{t-q}
$$
This is called ARIMA (p,d,q) model where, as before, p refers to the order of the autoregressive part, d refers to the degree of first differencing involved, and q is the order of the moving average part.

In order to apply the ARIMA model, as before we will use __fable__ package and we will look at different combinations of p being either 1 or 2, and q being either 1 or 2, while d always being 1.

```{r}
ARIMA_fit <- gold_train2 %>%
  model(arima111 = ARIMA(Price ~ pdq(1,1,1)), #ARIMA(1,1,1) model
        arima211 = ARIMA(Price ~ pdq(2,1,1)), #ARIMA(2,1,1) model
        arima112 = ARIMA(Price ~ pdq(1,1,2)), #ARIMA(1,1,2) model
        arima212 = ARIMA(Price ~ pdq(2,1,2))) #ARIMA(2,1,2) model

report(ARIMA_fit)
```

From the fitting of these 4 models, the AICc is lowest for ARIMA(2,1,2) model whereby we combine AR(2) and MA(2) for 1 differenced gold mini futures closing price data. We can fit this model as follows:

```{r}
ARIMA212_fit <- gold_train2 %>%
  model(ARIMA(Price ~ pdq(p=2, d=1, q=2)))

report(ARIMA212_fit)
```

The fitted model is therefore:

$$
y'_t = 0 - 1.4058 y'_{t-1} - 0.7773 y'_{t-2} + 1.3432 \varepsilon_{t-1} + 0.7515 \varepsilon_{t-2}
$$

We can plot the residuals to see if they are white noise:

```{r}
#gg_residuals() function will give us the residual plots and ACF plot

ARIMA212_fit %>%
  gg_tsresiduals()
```

We see that all except 1 residual are within the 95% interval around 0. And both count distribution and the residual plot shows that the residuals are likely to be white noise.

Lets check the forecasting accuracy next. we will first forecast to the next 565 days of our ARIMA(2, 1, 2) fitted model and then plot it against our test set to see how well we did:

```{r}
ARIMA_fc <- ARIMA212_fit %>%
  forecast(h=565) 

ARIMA_fc %>%
  autoplot(gold_train2, level = NULL) + #add training set's plot to our forecast. Setting level to NULL means it will not give us the default 95% and 80% intervals.
  autolayer(gold_test, Price, colour = "grey") + #add the layer of actual values from the test set
  labs(title = "ARIMA(2,1,2) Forecast for Gold Mini Closing Price") #add title

```

The forecast seems to be much worse than either of AR(2) or MA(2) models.  Let's see how we did in terms of RMSE and MAE:

```{r}
ARIMA_fc_accuracy <- accuracy(ARIMA_fc, gold_test)
ARIMA_fc_accuracy
```

We see that our typical error has increased to 4,481 INR for RMSE, and 3883 INR for MAE which is almost as poor as the accuracy we got with the Naive approach. Let's add this to our results table:

```{r}
fc_accuracy_results <- bind_rows(fc_accuracy_results,
                                 tibble(Method = "ARIMA(2,1,2)",
                                        ARIMA_fc_accuracy[,4:5]))
fc_accuracy_results
```









# References
Hyndman, R. J., & Koehler, A. B. (2006). Another look at measures of forecast accuracy. International Journal of Forecasting, 22(4), 679–688
Hyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 13 Sep 2022
Kwiatkowski, D., Phillips, P. C. B., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root: How sure are we that economic time series have a unit root? Journal of Econometrics, 54(1-3), 159–178. 
